# Attention mechanism
This repository looks to cover several of "well-known" approaches to attention mechanism. Methods that are being implemented are:

1. Global Attention

Based on paper [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025). It's focused on applying attention to Natural Language Translation using Seq2Seq model. The data set is simple English-French translation. 

2. Self Attention

TODO

3. Multi-head Attention

TODO

## Resources

TODO
[Stanford tutorial](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Original paper](https://arxiv.org/abs/1706.03762)
