# Transformer
PyTorch implementation of paper "Attention is all you need".


## Resources

[Stanford tutorial](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Original paper](https://arxiv.org/abs/1706.03762)

$`e^{-1}`$
